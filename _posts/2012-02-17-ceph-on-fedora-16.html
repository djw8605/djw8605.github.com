---
title: Ceph on Fedora 16
date: '2012-02-17T00:00:00.001-06:00'
author: Derek Weitzel
tags:
- ceph
- osg
modified_time: '2013-01-16T20:03:58.468-06:00'
thumbnail: http://1.bp.blogspot.com/-BnzkTQ2PVps/Tz3cbA7dTMI/AAAAAAAAA6E/xbEag3Nmn0c/s72-c/Screen+Shot+2012-02-16+at+10.49.26+PM.png
blogger_id: tag:blogger.com,1999:blog-3007054864987759910.post-8237054800561194348
blogger_orig_url: http://derekweitzel.blogspot.com/2012/02/ceph-on-fedora-16.html
---

I've written before how to run ceph on Fedora 15, but now I'm working on Fedora 16.<br /><br /><a href="http://derekweitzel.blogspot.com/2011/10/ceph-on-fedora-15.html">Last time</a>&nbsp;I complained about how much ceph tries to do for you. &nbsp;For better or worse, now it attempts to do more for you!<br /><br />For my setup, I had 3 nodes in the HCC private cloud. &nbsp;First, we need to install ceph.<br /><pre>$ yum install ceph<br /></pre><br />Then, create a configuration file for ceph. &nbsp;The RPM comes with a good example that my configuration is based on. &nbsp;The example script is in&nbsp;<span style="font-family: 'Courier New', Courier, monospace;">/usr/share/doc/ceph/sample.ceph.conf</span><br /><span style="font-family: 'Courier New', Courier, monospace;"><br /></span>My configuration:&nbsp;<a href="https://gist.github.com/1850697">Derek's Configuration</a><br /><br />The configuration has the authentication turned off. &nbsp;I found this useful because the ceph-authtool (yes, the renamed it since Fedora 15) is difficult to use. &nbsp;And because all of the nodes are on a private vlan only reachable by my openvpn key :)<br /><br />Then, you need to create and distribute ssh keys to all of your nodes so that the mkcephfs can ssh to them and configure.<br /><pre>$ ssh-keygen <br /></pre><br />Then copy them to the nodes:<br /><pre>$ ssh-copy-id i-000000c2<br />$ ssh-copy-id i-000000c3<br /></pre><br />Be sure to make the data directories on all the nodes. &nbsp;In this case:<br /><pre>$ mkdir -p /data/osd.0<br />$ ssh i-000000c2 'mkdir -p /data/osd.1'<br />$ ssh i-000000c3 'mkdir -p /data/osd.2'<br /></pre><br />Then run the mkcephfs command:<br /><pre>$ mkcephfs -a -c /etc/ceph/ceph.conf<br /></pre><br />And start up the daemons:<br /><pre>$ service ceph start<br /></pre><br />You should have the daemons running then. &nbsp;If they fail for some reason, they tend to output what the problem was. &nbsp;Also, the logs for the services are in /var/log/ceph<br /><br />To mount the filesystem, find an ip address of one of the monitors. &nbsp;In my case, I had a monitor on ip address&nbsp;10.148.2.147. &nbsp;The command to mount is:<br /><pre>$ mkdir -p /mnt/ceph<br />$ mount -t ceph 10.148.2.147:/ /mnt/ceph<br /></pre><br />Since you don't have any authentication, it should work without problems.<br /><br />I've had some problems with the different mds, even had a OSD die on me. &nbsp;It resolved itself, and I even added another OSD to take it's place, recreating the CRUSH table. &nbsp;Since creating this, I have even worked with the graphical interface:<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-BnzkTQ2PVps/Tz3cbA7dTMI/AAAAAAAAA6E/xbEag3Nmn0c/s1600/Screen+Shot+2012-02-16+at+10.49.26+PM.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="507" src="http://1.bp.blogspot.com/-BnzkTQ2PVps/Tz3cbA7dTMI/AAAAAAAAA6E/xbEag3Nmn0c/s640/Screen+Shot+2012-02-16+at+10.49.26+PM.png" width="640" /></a></div><br />And here's a presentation I did about the <a href="http://dl.acm.org/citation.cfm?id=1298485">CEPH Paper</a>. &nbsp;Note, &nbsp;I may not be entirely accurate in the presentation, do be kind.<br /><br /><script async class="speakerdeck-embed" data-id="4f3f0dbbb1f69c0022002626" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"></script>
