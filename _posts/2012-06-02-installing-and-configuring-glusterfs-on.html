---
title: Installing and Configuring glusterfs on EL6
date: '2012-06-02T09:30:00.000-05:00'
author: Derek Weitzel
tags:
- osg
modified_time: '2012-06-02T10:26:53.534-05:00'
blogger_id: tag:blogger.com,1999:blog-3007054864987759910.post-1723397805423890750
blogger_orig_url: http://derekweitzel.blogspot.com/2012/06/installing-and-configuring-glusterfs-on.html
---

I'm always interested in the newest technologies. &nbsp;With the<a href="http://www.redhat.com/promo/storage/"> purchase of Gluster</a> by RedHat, I figured it was time to give it a try. &nbsp;We are always looking for new technology that can lower the operations of our sysadmins, maybe Gluster is that option.<br /><br />This guide is heavily based on the <a href="http://www.gluster.org/wp-content/uploads/2012/05/Gluster_File_System-3.3.0-Administration_Guide-en-US.pdf">administrator's</a> guide for Gluster.<br /><br /><h3>             Installation</h3>All of the gluster packages are in EPEL, so first we need to install that repo on our nodes.<br /><pre class="rootscreen">$ rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-7.noarch.rpm</pre><br />Then install the glusterfs server:<br /><pre class="rootscreen">$ yum install glusterfs-server -y</pre><br />Then start the server:<br /><pre class="rootscreen">$ /etc/init.d/glusterd start</pre><br />For demo purposes only, flush the firewall: <br /><pre class="rootscreen">$ iptables -F</pre><h3>             Configuration</h3>And now add the nodes to the gluster system:<br /><pre class="rootscreen">$ gluster peer probe i-0000011a<br />Probe successful<br />$ gluster peer probe i-0000011c<br />Probe successful</pre><br />Now you can check for the nodes with the status command:<br /><br /><pre class="rootscreen">$ gluster peer status<br />Number of Peers: 2<br /><br />Hostname: i-0000011a<br />Uuid: 5bdc4f02-4e08-4794-af03-fd624be2d2e0<br />State: Peer in Cluster (Connected)<br /><br />Hostname: i-0000011c<br />Uuid: 248be1ba-c5aa-40d1-90e9-ca95a7e31697<br />State: Peer in Cluster (Connected)</pre><br />In this demo, I decided to make a&nbsp;Distributed Replicated volume. &nbsp;There are many options, but this seemed the best I could see.<br /><br />To create the volume:<br /><pre class="rootscreen">$ gluster volume create test-volume replica 3 transport tcp i-00000119:/exp1 i-0000011a:/exp2 i-0000011c:/exp3</pre><br />Note, I didn't make the directories /expX on any of the nodes, they are automatically made for you.<br /><br />To start the volume:<br /><pre class="rootscreen">$ gluster volume start test-volume</pre><br />To mount the volume, we don't have to modprobe fuse since it's built into the 2.6.32 kernel that comes with EL6. &nbsp;You can also use NFS to mount gluster volumes, but I decided to use fuse.<br /><pre class="rootscreen">$ mkdir -p /mnt/glusterfs<br />$ mount -t glusterfs i-0000011a:/test-volume /mnt/glusterfs</pre><br />YAY! working glusterfs. &nbsp;To confirm that it is working, I copied in a test file, mounted the test-volume on another node in the test cluster as well, and there was my file!<br /><br /><h3>             Summary</h3>GlusterFS doesn't seem too advanced compared to Hadoop or Ceph. &nbsp;If I look in the /expX directories I just see the whole file in there. &nbsp;In the current release, I believe the closest volume configuration we could have to Hadoop or Ceph is Striped Replicated Volumes. &nbsp;But, that volume type is only supported for use as a MapReduce backend.<br /><br />I think GlusterFS would be really cool for a OpenStack back end. &nbsp;Especially since it's so darn simple. &nbsp;Easily recoverable since the files are stored in plain text. &nbsp;Of course, you would probably want to do striping for the large image sizes of those files. <br /><br />Overall, I feel this was the easiest of the file systems I have tried out. &nbsp;Ceph was a little scary with all the configuration needed. &nbsp;GlusterFS was as simple as just issueing a command to add another server. &nbsp;Of course, does this mean it'll load balance the files if a server goes away? &nbsp;Don't really know how that'll work.<br /><br /><br />
