---
title: GPUs on the OSG
date: '2014-06-25T13:36:00.000-05:00'
author: Derek Weitzel
tags:
- osg
- condor
modified_time: '2014-06-25T13:54:51.577-05:00'
thumbnail: http://4.bp.blogspot.com/-hyEAcaZoDuI/U6saKtj1ToI/AAAAAAAACfQ/RnuWLeYzOSM/s72-c/Logo.png
blogger_id: tag:blogger.com,1999:blog-3007054864987759910.post-6396978665247392483
blogger_orig_url: http://derekweitzel.blogspot.com/2014/06/gpus-on-osg.html
---

<div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-hyEAcaZoDuI/U6saKtj1ToI/AAAAAAAACfQ/RnuWLeYzOSM/s1600/Logo.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://4.bp.blogspot.com/-hyEAcaZoDuI/U6saKtj1ToI/AAAAAAAACfQ/RnuWLeYzOSM/s1600/Logo.png" height="127" width="320" /></a></div><br />For a while, we have heard of the need for GPU's on the <a href="http://www.opensciencegrid.org/">Open Science Grid</a>. &nbsp;Luckily, HCC hosts numerous GPU resources that we are willing to share. &nbsp;And with the release today of <a href="http://research.cs.wisc.edu/htcondor/manual/v8.2.0/10_3Stable_Release.html">HTCondor 8.2</a>, we wanted to integrate OSG GPU resources transparently to the OSG.<br /><br /><h2>Submission</h2><div>Submission to the GPU resources at HCC uses the HTCondor-CE. &nbsp;The submission file is shown below (<a href="https://gist.github.com/djw8605/4022c3db54eb79f8b521">gist</a>):</div><div><script src="https://gist.github.com/djw8605/4022c3db54eb79f8b521.js"></script> </div><div><br /></div><div>You may notice that it is specifying Request_GPUs to specify the number of GPUs the job requires. This is the same command used when running with native (non-grid) HTCondor. &nbsp;You may submit with the line Request_GPUs = X (up to 3) to our Condor-CE node, as each GPU node has exactly 3 GPUs.<br /><br />Additionally, the OSG Glidein Factories have a new entry point, CMS_T3_US_Omaha_tusker_gpu, which is available for VO Frontends to submit GPU jobs. &nbsp;Email the glidein factory operators to enable GPU resources for your VO.<br /><br /></div><ul> </ul><h2>Job Structure</h2><div>The <a href="http://www.nvidia.com/object/cuda_home_new.html">CUDA</a> libraries are loaded automatically into the job's environment. &nbsp;Specifically, we are running CUDA libraries version 6.0.<br /><br />We tested submitting a binary compiled with 5.0 to Tusker. &nbsp;It required a wrapper script in order to configure the environment, and to transfer the CUDA library with the job. &nbsp;Details are on the <a href="https://gist.github.com/djw8605/56bba483c0b15461a589">gist</a> along with the example files.</div><div><br /></div><h2>Lessons Learned</h2><div>Things to note:<br /><ul><li>If a job matches more than 1 HTCondor-CE route, then the router will round robin between the routes. &nbsp;Therefore, it is necessary to modify all routes if you wish specific jobs to go to a specific route.</li><li>Grid jobs do not source /etc/profile.d/ on the worker node. &nbsp;I had to manually source those files in the <a href="https://gist.github.com/djw8605/88fdc897a9cd2a839328">pbs_local_submit_attributes.sh</a> file in order to use the module command and load the CUDA environment.</li></ul><div><br /></div><h2>Resources</h2></div><div><ul><li>HTCondor-CE Job Router <a href="https://gist.github.com/djw8605/8540553f0869c7a1ed14">config</a> in order to route GPU jobs appropriately.</li><li>HTCondor-CE PBS local submit file attributes <a href="https://gist.github.com/djw8605/88fdc897a9cd2a839328">file</a> that includes the source and module commands.</li></ul></div>
